{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b798e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedant\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\vedant\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vedant\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vedant\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\vedant\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c96e8b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_precision_recall_curve' from partially initialized module 'sklearn.metrics' (most likely due to a circular import) (C:\\Users\\Vedant\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# from sklearn.model_selection import train_test_split\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.metrics` module includes score functions, performance metrics\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mand pairwise metrics and distance computations.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_precision_recall_curve\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ranking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auc\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ranking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m average_precision_score\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_precision_recall_curve' from partially initialized module 'sklearn.metrics' (most likely due to a circular import) (C:\\Users\\Vedant\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Make necessary imports.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ad44a",
   "metadata": {},
   "source": [
    "# PREPROCCESING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset:-\n",
    "df = pd.read_csv(r'C:\\Users\\Vedant\\Desktop\\PBL\\kidney_disease.csv')\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da96bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape  #to check whether teh data set loaded properly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the null values in the dataset\n",
    "df.isnull().sum()  #gives how many null values per columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing null values\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mode = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "\n",
    "df_imputed = pd.DataFrame(imp_mode.fit_transform(df))\n",
    "df_imputed.columns = df.columns\n",
    "df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa64a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see now there is no null values\n",
    "df_imputed.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.columns   #Gives the list of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0268595",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_imputed[\"age\"].tolist())   #tolist- to convert into list, and set()- to keep only unique values in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7169b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the unique values in every column.\n",
    "\n",
    "for i in df_imputed.columns:\n",
    "    print(\"*****************************************************\",i,\"*********************************************************\")\n",
    "    print()\n",
    "    print(set(df_imputed[i].tolist()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c024d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In dataset we found that there are many mistaken values, so we replacing mistaken values with actual values.\n",
    "\n",
    "print(df_imputed[\"rc\"].mode())\n",
    "print(df_imputed[\"wc\"].mode())\n",
    "print(df_imputed[\"pcv\"].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we using if/else inline function to replace values.\n",
    "\n",
    "df_imputed[\"classification\"]=df_imputed[\"classification\"].apply(lambda x:'ckd' if x==\"ckd\\t\" else x)\n",
    "\n",
    "df_imputed[\"cad\"]=df_imputed[\"cad\"].apply(lambda x:'no' if x==\"\\tno\" else x)\n",
    "\n",
    "df_imputed[\"dm\"]=df_imputed[\"dm\"].apply(lambda x:'no' if x==\"\\tno\" else x)\n",
    "df_imputed[\"dm\"]=df_imputed[\"dm\"].apply(lambda x:'yes' if x==\"\\tyes\" else x)\n",
    "df_imputed[\"dm\"]=df_imputed[\"dm\"].apply(lambda x:'yes' if x==\"yes\" else x)\n",
    "\n",
    "df_imputed[\"rc\"]=df_imputed[\"rc\"].apply(lambda x:'5.2' if x==\"\\t?\" else x)\n",
    "\n",
    "df_imputed[\"wc\"]=df_imputed[\"wc\"].apply(lambda x:'9800' if x==\"\\t6200\" else x)\n",
    "df_imputed[\"wc\"]=df_imputed[\"wc\"].apply(lambda x:'9800' if x==\"\\t8400\" else x)\n",
    "df_imputed[\"wc\"]=df_imputed[\"wc\"].apply(lambda x:'9800' if x==\"\\t?\" else x)\n",
    "\n",
    "df_imputed[\"pcv\"]=df_imputed[\"pcv\"].apply(lambda x:'41' if x==\"\\t43\" else x)\n",
    "df_imputed[\"pcv\"]=df_imputed[\"pcv\"].apply(lambda x:'41' if x==\"\\t?\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that all the errors are been removed.\n",
    "\n",
    "for i in df_imputed.columns:\n",
    "    print(\"*****************************************************\",i,\"*********************************************************\")\n",
    "    print()\n",
    "    print(set(df_imputed[i].tolist()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_imputed[\"classification\"].value_counts()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Label Imbalance:- \n",
    "#label imbalance -uneven distribution of classes in a classification problem, where one or more classes have significantly fewer instances compared to others.\n",
    "\n",
    "\n",
    "temp = df_imputed[\"classification\"].value_counts()\n",
    "temp_df = pd.DataFrame({'classification': temp.index, 'values': temp.values})\n",
    "print(sns.barplot(x= 'classification', y =\"values\", data = temp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing datatypes\n",
    "\n",
    "df_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab20b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluding datatype 'object'\n",
    "\n",
    "df.select_dtypes(exclude = [\"object\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing datatypes 'object' to 'float'.\n",
    "\n",
    "for i in df.select_dtypes(exclude = [\"object\"]).columns:\n",
    "    df_imputed[i] = df_imputed[i].apply(lambda x: float(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b058d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA and Visualizations:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8859ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_imputed)    #Plots the relation b/w the pair of the attributes.\n",
    "\n",
    "# scatters - numerical columns\n",
    "# straight lines - categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d83c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the distribution of data.\n",
    "\n",
    "def distplots(col):\n",
    "    sns.distplot(df[col])\n",
    "    plt.show()\n",
    "    \n",
    "for i in (df.select_dtypes(exclude = [\"object\"]).columns)[1:]:\n",
    "    distplots(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding and removing the outliers:-\n",
    "#Outliers- Outliers are data points that are significantly different from the majority of the data.\n",
    "#They can be unusually high or low values compared to the rest of the dataset. Outliers can occur due to errors, rare events,\n",
    "#or natural variations. \n",
    "\n",
    "def boxplots(col):\n",
    "    sns.boxplot(df[col])\n",
    "    plt.show()\n",
    "    \n",
    "for i in (df.select_dtypes(exclude = [\"object\"]).columns)[1:]:\n",
    "    boxplots(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL PART OF PREPROCCESING:-\n",
    "#Encoding the values,that means there will be no 'yes' or 'no' in the data set. ALl the values are replace by '0' and '1' and \n",
    "# Numerical or integer values remains same.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_enco = df_imputed.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "df_enco\n",
    "\n",
    "#As we can see, all the values are get replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ff2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE COMPLETED ENITRE PREPROCESSING PART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2672775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enco.to_csv(\"kidney_disease_pre-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Correlations\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "corr = df_enco.corr()\n",
    "sns.heatmap(corr,annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec7825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make the final changes to the data.\n",
    "#Seperate the independent data(x) and dependent data(y)\n",
    "#dependent data/attribute in 'classification' which is dependent on other attributes. And data other than 'classification'\n",
    "#independent data.\n",
    "\n",
    "#Now, dropping 'id' columns from data bcoz we don't have need.\n",
    "\n",
    "x = df_enco.drop([\"id\", \"classification\"], axis=1)\n",
    "y = df_enco[\"classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69aa960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets detect the label balance\n",
    "import collections\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets balance the label \n",
    "\n",
    "ros = RandomOverSampler()\n",
    "x_ros, y_ros = ros.fit_resample(x,y)\n",
    "print(Counter(y_ros))\n",
    "\n",
    "#As we can see, labels are balanced now.\n",
    "#means ckd = 250 , notckd = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4737c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the MinMaxScaler and scale the features b/w -1 and 1 to normalize them.\n",
    "#The MinMaxScaler transform the features by scaling them to a given range.\n",
    "#The fit_transform() method fits to the data and then transform it. We dont need to scale the labels\n",
    "#Scale the features to b/w -1 and 1.\n",
    "\n",
    "#Scaling is important in the algorithm such as support vector machine(SVM) and k-nearest neighbout(KNN) where the distance b/w \n",
    "#the data point is important.\n",
    "\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "x = scaler.fit_transform(x_ros)\n",
    "y = y_ros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b808dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality reduction\n",
    "#Applying PCA\n",
    "\n",
    "#The code below is .95 for the number of component parameter.\n",
    "#It means that Skcit-learn choose the minimum number of principal components such that 95% of variance is retained.\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(.95)\n",
    "X_PCA = pca.fit_transform(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(X_PCA.shape)\n",
    "#Thus we need 18 columns to keep 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With PCA \n",
    "#Now split the dataset into training and testing sets keeping 20% of the data for the testing.\n",
    "#Split the dataset.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_PCA, y,test_size=0.2, random_state=7)\n",
    "#A sequential model is appropriate for a plain stack of layers where each layer has exactly one inout and one ouput  and one output is connected to the another input\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoints, EarlyStopping\n",
    "# from keras.models import Sequential, Model\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aafc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd568b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential, Model\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ff2a8d2",
   "metadata": {},
   "source": [
    "#Creating the model:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(15, input_shape = (x_train.shape[1],), activation = 'relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(Dense(15, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.4))\n",
    "    classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ad018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()     #To be check \n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81946228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c315a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikitplot\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, plot_precision_recall_curve, f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot the validation curve.\n",
    "\n",
    "def plot_auc(t_y, p_y):\n",
    "    fpr, tpr, thresholds = roc_curve(t_y, p_y, pos_label = 1)\n",
    "    fig, c_ax = plt.subplots(1,1,figsize = (9,9))\n",
    "    c_ax.plot(fpr, tpr, label = '%s(AUC:0.2f)' % ('classification', auc(fpr,tpr)))\n",
    "    c_ax.plot([0,1], [0,1], color = 'navy', lw = 1, linestyle = '==')\n",
    "    c_ax.legend()\n",
    "    c_ax.set_xlabel('false positive rate')\n",
    "    c_ax.set_ylabel('true positive rate')\n",
    "    \n",
    "#function to plot the precision recall curve.You can utilize at precision_recall_curve imported above.\n",
    "def plot_precision_recall_helper(t_y, p_y):\n",
    "    fig, c_ax = plt.subplots(1,1,figsize = (9,9))\n",
    "    precision, recall, thresholds = precision_recall_curve(t_y, p_y, pos_label = 1)\n",
    "    aps = averge_precision_score(t_y, p_y)\n",
    "    c_ax.plot(recall,precision, label = '%s(AUC:0.2f)' % ('classification', aps))\n",
    "    c_ax.plot(recall, precision, color = 'red', lw = 1)\n",
    "    c_ax.legend()\n",
    "    c_ax.set_xlabel('recall')\n",
    "    c_ax.set_ylabel('precision')\n",
    "    \n",
    "#function to plot the history\n",
    "def plot_history(history):\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(15)\n",
    "    \n",
    "    f.add_subplot(1,2,1)\n",
    "    plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "    plt.plot(history.history['loss'], label = 'train loss')\n",
    "    plt.legend()\n",
    "    plt.title('Model loss')\n",
    "    \n",
    "    f.add_subplot(1,2,2)\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val accuracy')\n",
    "    plt.plot(history.history['accuracy'], label = 'train accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Model Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(y_test, model.predict(x_test, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164a1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
